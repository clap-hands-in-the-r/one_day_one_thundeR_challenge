
# Loading packages required

```{r}
library(dslabs)
library(caret)
library(tidyverse)
library(purrr)

```


# First approach of prediction

```{r}
data("heights")
head(heights)
# Sex will be the outcome we want to predict
# and height will be our predictor / our feature

# we call the outcome y and the predictor x

y <- heights$sex
x <- heights$height

# Caret package contains a function that helps us to create 
# a test set and a train set

set.seed(2007)

test_index <- createDataPartition(y,times =1, p =0.5, list = FALSE)
test_set <- heights[test_index,]
train_set <- heights[-test_index,]

# predict the sex by randomly guessing and measure the accuracy
# we'll call our guessing, our prediction y_hat

# caret package enables computation of accuracy etc on outcome that comes on the form
# of a factor
y_hat <- sample(c("Male","Female"), size = length(test_index), replace = TRUE) |> factor(c("Male","Female"))

# Measurement of overall accuracy
mean(y_hat == test_set$sex) # [1] 0.5161905

# Can we do better than guessing ?

summary(heights)
my_summary <- heights |> 
    group_by(sex) |>
    summarise(Mean = mean(height), Median = median(height), SD = sd(height))


#   sex     Mean Median    SD
#   <fct>  <dbl>  <dbl> <dbl>
# 1 Female  64.9   65.0  3.76
# 2 Male    69.3   69    3.61

heights |> 
    summarise(Mean = mean(height), Median = median(height), SD = sd(height))
# Male height minus 2 standard deviations from the mean

my_cutoff <- my_summary |> filter(sex == "Male") |> mutate(my_cutoff = Mean - 2 * SD) |> pull(my_cutoff)

y_hat <- ifelse(test_set$height > my_cutoff, "Male","Female") |> factor(levels = levels(test$sex))

# Compute the new overall accuracy given our new algorithm
mean(y_hat == test_set$sex) [1] 0.7809524

```

# Best cutoff

```{r}
# Could we improve our cutoff

# we must do our exploratories on the train set 
# and keep the test set for confirmation

# we are going to try cutoffs from 61 to 69 (69 is the median height for males)

cutoff <- seq(61,70)

accuracy <- map_dbl(cutoff, function(x){
    y_hat <- ifelse(train_set$height > x, "Male", "Female") |> factor(levels = levels(train_set$sex))
    mean(y_hat == train_set$sex)
})

df <- data.frame(cutoff = cutoff, accuracy = accuracy)

# Visualize accuracy according to cutoff
ggplot(df, mapping = aes(x = cutoff, y = accuracy)) + geom_point() + geom_line()


max(accuracy) # [1] 0.8495238
cutoff[which.max(accuracy)] # [1] 64
# the maximum accuracy reached is 0.84 for a cutoff of 64

# apply this cutoff to the test set in order to compute accuracy

y_hat <- ifelse(test_set$height > 64, "Male", "Female") |> factor(levels = levels(test_set$sex))
mean(y_hat == test_set$sex) # [1] 0.8038095

# overall accuracy observed on the test set is a bit lower but is still really good

```

# Is overall accuracy allways relevant ?

```{r}
# doesn'it seem strange that best cutoff is a height of 64 whereas
# 64 is also really close from the mean of the height in the heights dataset?

#   sex     Mean Median    SD
#   <fct>  <dbl>  <dbl> <dbl>
# 1 Female  64.9   65.0  3.76
# 2 Male    69.3   69    3.61

# to understand the fact we can built tabulate each combination of predicted and actual value
# what is known as the confusion matrix

table(predicted = y_hat, actual = test_set$sex)

         # actual
# predicted Female Male
#    Female     48   32
#    Male       71  374

# if we compute the accuracy for each sex, we can observe a problem

test_set  <- cbind(test_set, y_hat = y_hat)

test_set |> 
    group_by(sex) |> 
    summarise(accuracy_by_sex = mean(y_hat == sex))

#   sex    accuracy_by_sex
#   <fct>            <dbl>
# 1 Female           0.403
# 2 Male             0.921


# actually we can observe an accuracy of 0.40 for women
# this bad measurement is outwheigted because of the low prevalance of women in the dataset

prev <- mean(heights$sex == "Female")
prev # [1] 0.2266667

```

# Sensitivity and specificity

```{r}
# To improve the measurement of how an algorithm is good to predict the actual outcome
# we can add some other indicators
# We can start again with the confusion matrix

table(predicted = y_hat, actual =test_set$sex)


#          actual
# predicted Female Male
#    Female     48   32
#    Male       71  374



#         actual |
# predicted      |y = 1                  | y = 0
# -----------------------------------------------------------
# y_hat = 1      |True Positives(TP)     | False Positives(FP)
# y_hat = 0      |False Negatives(FN)    | True Negatives(TN)

# we need a binary outcome to build such a matrix
# the choice of what is considered positive outcome y = 1 is arbitrary
# because the logical behind is symetric

# in the heights dataset case, y = 1 (positive outcome ) is sex is equal to female
# this is just technical because we need the outcome to be a factor
# and R ranks "Female" before "Male" just because alphabetical order

# confusionMatrix is a function from the caret package

cm <- confusionMatrix(data = y_hat, reference = test_set$sex)
str(cm)
cm$byClass

cm$byClass[c("Sensitivity", "Specificity","Prevalence")]
# Sensitivity Specificity  Prevalence 
#   0.4033613   0.9211823   0.2266667 

cm$overall["Accuracy"]
#  Accuracy 
# 0.8038095 

# Confusion matrix gives us a lot of metrics

# For the two following metrics we compute by column

# Given the table with TP / FP / FP / TN, 
# sensitivity is called the True Positive Rate (TPR)
# The formula is TPR = TP / (TP +  FN)
48/(48 + 71) # [1] 0.4033613 sensitivty is also called Recall

# specificity is called True negative rate (TNR) 
# and the formula is TNR = TN / (FP + TN)
374/(32 +374) # [1] 0.9211823 this measurement of specificity is also called 1-FPR

# Another measurment of specificity is called precision
# TP / (TP + FN)
48 / (48 + 32 ) # [1] 0.6
# precision is also called positive predicted value (PPV)
# and one last name is Precision (the name given in the confusion matrix)
# we must notice this ratio is impacted by the prevalence
# precision depends on prevalence since higher prevalence implies 
# you can get higher precision even by guessing 

```

# Balanced accuracy

```{r}
# it's important to watch to both indicators sensitivity and specificity
# but it could also be useful to have only one metric to measure both 
# that's why we have a new indicator, the balanced accuracy or FMEAS
# that will be given by the formula below

#1 / ( 1/2 * recall + 1/2 * precision)

# if we want to give a different weight to recall and to precision,
# we can use a weighted harmonic average
# 1/ ( Beta/(1+ Beta) *  recall + 1/(1 + Beta) * precision)
# the F_meas function in the caret package give a weight of 1 to Beta by default

cutoff <- seq(61,70)

F_1 <- map_dbl(cutoff, function(x){
    y_hat <- ifelse(train_set$height > x, "Male","Female") |> factor(levels = levels(test_set$sex))
    F_meas(data = y_hat, reference = train_set$sex)
}
)

vis <- data.frame(cutoff = cutoff, F_meas = F_1)
ggplot(vis, mapping = aes(x = cutoff, y = F_1)) +
    geom_point() +
    geom_line()

max(F_1) # [1] 0.6470588
# we get a max F_meas of 0.64
cutoff[which.max(F_1)]
# for a cutoff of 66

# in the previous optimization we based on overall accuracy, we determined a cutoff of 64
# but 66 will give a best balance between sensitivity and specificity


```

# Summary and comparison of the two previous models

```{r}
# we label mod1 the model 1 with a cutoff at 64
y_mod1 <- ifelse(test_set$height > 64, "Male", "Female") |> factor(levels = levels(test_set$sex))
cm_mod1 <- confusionMatrix(data = y_mod1, reference = test_set$sex)


# we label mod2 the model 2 with a cutoff at 66
y_mod2 <- ifelse(test_set$height > 66, "Male", "Female") |> factor(levels = levels(test_set$sex))
cm_mod2<- confusionMatrix(data = y_mod2, reference = test_set$sex)

cm_mod1$overall["Accuracy"] # 0.8038095 
cm_mod2$overall["Accuracy"] # 0.7866667 

cm_mod1$byClass[c("Sensitivity","Specificity","Precision","F1","Balanced Accuracy")]
      # Sensitivity       Specificity         Precision                F1 Balanced Accuracy 
      #   0.4033613         0.9211823         0.6000000         0.4824121         0.6622718

cm_mod2$byClass[c("Sensitivity","Specificity","Precision","F1","Balanced Accuracy")]
      # Sensitivity       Specificity         Precision                F1 Balanced Accuracy 
      #   0.6302521         0.8325123         0.5244755         0.5725191         0.7313822 

# we accept a lower overall accuracy in favor of a greater balanced accuracy

```

# ROC (receiver operating characteristic) curves

```{r}
# in our two previous models, predicting through a cutoff in the height clearly outperformed
# method of guessing, but we only tested guessing with a prob of 0.5
# we will now try with guessing with a prob of 0.9


p1 <- 0.9
y_hat_dot9 <- sample(c("Male","Female"), size = length(test_index), replace = TRUE, prob = c(p1, 1-p1)) |> 
    factor(levels = levels(test_set$sex))
mean(y_hat_dot9 == test_set$sex) # [1] 0.7219048
# we get a far better accuracy than with .5

p2 <- 0.5
y_hat_dot5 <- sample(c("Male","Female"), size = length(test_index), replace = TRUE, prob = c(p2, 1-p2)) |> 
    factor(levels = levels(test_set$sex))
mean(y_hat_dot5 == test_set$sex) # [1] 0.512381

cm <- function(x) {confusionMatrix(data = x, reference = test_set$sex)}

cm_dot5 <- cm(y_hat_dot5)
cm_dot9 <- cm(y_hat_dot9)

cm_dot5$overall["Accuracy"]
# Accuracy 
# 0.512381 

cm_dot9$overall["Accuracy"]
# Accuracy 
# 0.727619

cm_dot5$byClass[c("Sensitivity","Specificity")]
# Sensitivity Specificity 
#   0.5378151   0.5049261 

cm_dot9$byClass[c("Sensitivity","Specificity")]
# Sensitivity Specificity 
#   0.1092437   0.9088670 

# we see that the best accuracy we gain with .9 comes at the cost of falling sensitivity

# we can plot the datas in order to see how we balance sensitivity and specificity

probs <- seq(0,1, by = 0.1)
n <- length(test_index)


guessing_meth <- map_df(probs,
                   function(p){
                      y_hat <-  sample(c("Male","Female"), size = n, replace = TRUE, prob = c(p,1-p))|>
                          factor(levels = c("Female","Male"))
                       list(
                           method = "guessing",
                           cutoff = p,
                           FPR = 1 - specificity(y_hat, test_set$sex),
                           TPR = sensitivity(y_hat, test_set$sex)
                       )
                       
                   })

ggplot(guessing_meth, mapping = aes(x = FPR, y = TPR)) +
    geom_point() +
    geom_line() +
    geom_text(
        label = guessing_meth$cutoff,
        # nudge_x = 0.25,
        nudge_y = -0.09
        # check_overlap = T
    )


the_height <- c(50,seq(59,75, by =1), 80)
height_meth <- map_df(the_height, function(x){
    y_hat <- ifelse(test_set$height > x, "Male","Female") |> factor(levels = levels(test_set$sex))
    list(
        method = "height",
        
        cutoff = x,
        FPR = 1 - specificity(y_hat, test_set$sex),
        TPR = sensitivity(y_hat, test_set$sex)
    )
    
}
)

comparing_2_meth <- rbind(guessing_meth,height_meth)


ggplot(comparing_2_meth, aes(x = FPR, y = TPR, colour = method))+
    geom_point()+
    geom_line()+
    geom_text(label=comparing_2_meth$cutoff, nudge_y = 0.09)

# We can see that for each value of False positive rate (FPR), 
# the true positive rate (TPR) of height cutoff method is better than 
# the TPR with the guessing method


# understanding deeper TPR and FPR with exemple of an extreme cutoff seize 80

y_hat <- ifelse(test_set$height > 80, "Male", "Female" ) |> factor(levels = levels(test_set$sex))
cm <- confusionMatrix(data = y_hat, reference = test_set$sex)
cm$overall["Accuracy"]
#  Accuracy 
# 0.2304762
cm$byClass[c("Sensitivity", "Specificity")]
# Sensitivity Specificity 
# 1.000000000 0.004926108 

# we see that sensitvity ie the probability of predicting positive when actually positive
# is 100% but specificity ie the probability of prediction negative when actually negative
# is absolutely very very low / in our case study, we recall that given female comes before male in 
# alphabetical order, predicting positive is predicting female and predicting negative is predicting female


table(predicted = y_hat, actual = test_set$sex)
#          actual
# predicted Female Male
#    Female    119  404
#    Male        0    2

# given our very high height cutoff, we only predict two individuals 
# as male
# that's why precision (in this case, predicting male when actual male = True negatives) is 
# very low
# we can recalculate our specifity manually with formula TN/(TN + FP)
2 /( 404 +2)
# [1] 0.004926108 
# and our false positive rate is very very high and close to 1
# we are almost certain to be wrong when predicting male with this cutoff

```

