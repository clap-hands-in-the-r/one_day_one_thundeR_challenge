
# Loading packages required

```{r}
library(dslabs)
library(caret)
library(tidyverse)
library(purrr)

```


# First approach of prediction

```{r}
data("heights")
head(heights)
# Sex will be the outcome we want to predict
# and height will be our predictor / our feature

# we call the outcome y and the predictor x

y <- heights$sex
x <- heights$height

# Caret package contains a function that helps us to create 
# a test set and a train set

set.seed(2007)

test_index <- createDataPartition(y,times =1, p =0.5, list = FALSE)
test_set <- heights[test_index,]
train_set <- heights[-test_index,]

# predict the sex by randomly guessing and measure the accuracy
# we'll call our guessing, our prediction y_hat

# caret package enables computation of accuracy etc on outcome that comes on the form
# of a factor
y_hat <- sample(c("Male","Female"), size = length(test_index), replace = TRUE) |> factor(c("Male","Female"))

# Measurement of overall accuracy
mean(y_hat == test_set$sex) # [1] 0.5161905

# Can we do better than guessing ?

summary(heights)
my_summary <- heights |> 
    group_by(sex) |>
    summarise(Mean = mean(height), Median = median(height), SD = sd(height))


#   sex     Mean Median    SD
#   <fct>  <dbl>  <dbl> <dbl>
# 1 Female  64.9   65.0  3.76
# 2 Male    69.3   69    3.61

heights |> 
    summarise(Mean = mean(height), Median = median(height), SD = sd(height))
# Male height minus 2 standard deviations from the mean

my_cutoff <- my_summary |> filter(sex == "Male") |> mutate(my_cutoff = Mean - 2 * SD) |> pull(my_cutoff)

y_hat <- ifelse(test_set$height > my_cutoff, "Male","Female") |> factor(levels = levels(test$sex))

# Compute the new overall accuracy given our new algorithm
mean(y_hat == test_set$sex) [1] 0.7809524

```

# Best cutoff

```{r}
# Could we improve our cutoff

# we must do our exploratories on the train set 
# and keep the test set for confirmation

# we are going to try cutoffs from 61 to 69 (69 is the median height for males)

cutoff <- seq(61,70)

accuracy <- map_dbl(cutoff, function(x){
    y_hat <- ifelse(train_set$height > x, "Male", "Female") |> factor(levels = levels(train_set$sex))
    mean(y_hat == train_set$sex)
})

df <- data.frame(cutoff = cutoff, accuracy = accuracy)

# Visualize accuracy according to cutoff
ggplot(df, mapping = aes(x = cutoff, y = accuracy)) + geom_point() + geom_line()


max(accuracy) # [1] 0.8495238
cutoff[which.max(accuracy)] # [1] 64
# the maximum accuracy reached is 0.84 for a cutoff of 64

# apply this cutoff to the test set in order to compute accuracy

y_hat <- ifelse(test_set$height > 64, "Male", "Female") |> factor(levels = levels(test_set$sex))
mean(y_hat == test_set$sex) # [1] 0.8038095

# overall accuracy observed on the test set is a bit lower but is still really good

```

# Is overall accuracy allways relevant ?

```{r}
# doesn'it seem strange that best cutoff is a height of 64 whereas
# 64 is also really close from the mean of the height in the heights dataset?

#   sex     Mean Median    SD
#   <fct>  <dbl>  <dbl> <dbl>
# 1 Female  64.9   65.0  3.76
# 2 Male    69.3   69    3.61

# to understand the fact we can built tabulate each combination of predicted and actual value
# what is known as the confusion matrix

table(predicted = y_hat, actual = test_set$sex)

         # actual
# predicted Female Male
#    Female     48   32
#    Male       71  374

# if we compute the accuracy for each sex, we can observe a problem

test_set  <- cbind(test_set, y_hat = y_hat)

test_set |> 
    group_by(sex) |> 
    summarise(accuracy_by_sex = mean(y_hat == sex))

#   sex    accuracy_by_sex
#   <fct>            <dbl>
# 1 Female           0.403
# 2 Male             0.921


# actually we can observe an accuracy of 0.40 for women
# this bad measurement is outwheigted because of the low prevalance of women in the dataset

prev <- mean(heights$sex == "Female")
prev # [1] 0.2266667

```

# Sensitivity and specificity

```{r}
# To improve the measurement of how an algorithm is good to predict the actual outcome
# we can add some other indicators
# We can start again with the confusion matrix

table(predicted = y_hat, actual =test_set$sex)


#          actual
# predicted Female Male
#    Female     48   32
#    Male       71  374



#         actual |
# predicted      |y = 1                  | y = 0
# -----------------------------------------------------------
# y_hat = 1      |True Positives(TP)     | False Negatives(FP)
# y_hat = 0      |False Positives(FP)    | True Negatives(TN)

# we need a binary outcome to build such a matrix
# the choice of what is considered positive outcome y = 1 is arbitrary
# because the logical behind is symetric

# in the heights dataset case, y = 1 (positive outcome ) is sex is equal to female
# this is just technical because we need the outcome to be a factor
# and R ranks "Female" before "Male" just because alphabetical order

# confusionMatrix is a function from the caret package

cm <- confusionMatrix(data = y_hat, reference = test_set$sex)
str(cm)
cm$byClass

cm$byClass[c("Sensitivity", "Specificity","Prevalence")]
# Sensitivity Specificity  Prevalence 
#   0.4033613   0.9211823   0.2266667 

cm$overall["Accuracy"]
#  Accuracy 
# 0.8038095 

# Confusion matrix gives us a lot of metrics

# For the two following metrics we compute by column

# Given the table with TP / FP / FP / TN, 
# sensitivity is called the True Positive Rate (TPR)
# The formula is TPR = TP / (TP +  FP)
48/(48 + 71) # [1] 0.4033613 sensitivty is also called Recall

# specificity is called True negative rate (TNR) 
# and the formula is TNR = TN / (FN + TN)
374/(32 +374) # [1] 0.9211823 this measurement of specificity is also called 1-FPR

# Another measurment of specificity is called precision
# TP / (TP + FN)
48 / (48 + 32 ) # [1] 0.6
# precision is also called positive predicted value (PPV)
# and one last name is Precision (the name given in the confusion matrix)
# we must notice this ratio is impacted by the prevalence
# precision depends on prevalence since higher prevalence implies 
# you can get higher precision even by guessing 

```

