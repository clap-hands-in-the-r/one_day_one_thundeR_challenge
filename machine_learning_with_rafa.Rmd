
# Loading packages required

```{r}
library(dslabs)
library(caret)
library(tidyverse)
library(purrr)

```


# First approach of prediction

```{r}
data("heights")
head(heights)
# Sex will be the outcome we want to predict
# and height will be our predictor / our feature

# we call the outcome y and the predictor x

y <- heights$sex
x <- heights$height

# Caret package contains a function that helps us to create 
# a test set and a train set

set.seed(2007)

test_index <- createDataPartition(y,times =1, p =0.5, list = FALSE)
test_set <- heights[test_index,]
train_set <- heights[-test_index,]

# predict the sex by randomly guessing and measure the accuracy
# we'll call our guessing, our prediction y_hat

# caret package enables computation of accuracy etc on outcome that comes on the form
# of a factor
y_hat <- sample(c("Male","Female"), size = length(test_index), replace = TRUE) |> factor(c("Male","Female"))

# Measurement of overall accuracy
mean(y_hat == test_set$sex) # [1] 0.5161905

# Can we do better than guessing ?

summary(heights)
my_summary <- heights |> 
    group_by(sex) |>
    summarise(Mean = mean(height), Median = median(height), SD = sd(height))


#   sex     Mean Median    SD
#   <fct>  <dbl>  <dbl> <dbl>
# 1 Female  64.9   65.0  3.76
# 2 Male    69.3   69    3.61

heights |> 
    summarise(Mean = mean(height), Median = median(height), SD = sd(height))
# Male height minus 2 standard deviations from the mean

my_cutoff <- my_summary |> filter(sex == "Male") |> mutate(my_cutoff = Mean - 2 * SD) |> pull(my_cutoff)

y_hat <- ifelse(test_set$height > my_cutoff, "Male","Female") |> factor(levels = levels(test$sex))

# Compute the new overall accuracy given our new algorithm
mean(y_hat == test_set$sex) [1] 0.7809524

```

# Best cutoff

```{r}
# Could we improve our cutoff

# we must do our exploratories on the train set 
# and keep the test set for confirmation

# we are going to try cutoffs from 61 to 69 (69 is the median height for males)

cutoff <- seq(61,70)

accuracy <- map_dbl(cutoff, function(x){
    y_hat <- ifelse(train_set$height > x, "Male", "Female") |> factor(levels = levels(train_set$sex))
    mean(y_hat == train_set$sex)
})

df <- data.frame(cutoff = cutoff, accuracy = accuracy)

# Visualize accuracy according to cutoff
ggplot(df, mapping = aes(x = cutoff, y = accuracy)) + geom_point() + geom_line()


max(accuracy) # [1] 0.8495238
cutoff[which.max(accuracy)] # [1] 64
# the maximum accuracy reached is 0.84 for a cutoff of 64

# apply this cutoff to the test set in order to compute accuracy

y_hat <- ifelse(test_set$height > 64, "Male", "Female") |> factor(levels = levels(test_set$sex))
mean(y_hat == test_set$sex) # [1] 0.8038095

# overall accuracy observed on the test set is a bit lower but is still really good

```

# Is overall accuracy allways relevant ?

```{r}
# doesn'it seem strange that best cutoff is a height of 64 whereas
# 64 is also really close from the mean of the height in the heights dataset?

#   sex     Mean Median    SD
#   <fct>  <dbl>  <dbl> <dbl>
# 1 Female  64.9   65.0  3.76
# 2 Male    69.3   69    3.61

# to understand the fact we can built tabulate each combination of predicted and actual value
# what is known as the confusion matrix

table(predicted = y_hat, actual = test_set$sex)

         # actual
# predicted Female Male
#    Female     48   32
#    Male       71  374

# if we compute the accuracy for each sex, we can observe a problem

test_set  <- cbind(test_set, y_hat = y_hat)

test_set |> 
    group_by(sex) |> 
    summarise(accuracy_by_sex = mean(y_hat == sex))

#   sex    accuracy_by_sex
#   <fct>            <dbl>
# 1 Female           0.403
# 2 Male             0.921


# actually we can observe an accuracy of 0.40 for women
# this bad measurement is outwheigted because of the low prevalance of women in the dataset

prev <- mean(heights$sex == "Female")
prev # [1] 0.2266667

```

